{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import display\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from multiprocessing import Process, Pool\n",
    "import multiprocessing\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "import ctypes\n",
    "import datetime\n",
    "from math import sqrt\n",
    "import platform\n",
    "platform.architecture()\n",
    "# %load_ext line_profiler\n",
    "import scipy\n",
    "import os\n",
    "# %load_ext Cython\n",
    "now = lambda: datetime.datetime.now()\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nacteni dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "directory = 'sample-data-v2/'\n",
    "ndataset = 'last-fm-2k-new'#\"movielens_1m\"\n",
    "data_path = '/home/kuba/ownCloud/Recombee/'\n",
    "\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/items.json\",'r') as f:\n",
    "    items = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/properties.json\",'r') as f:\n",
    "    properties = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/user.folds.json\",'r') as f:\n",
    "    user_folds = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/users.int2str.json\",'r') as f:\n",
    "    users_int2str = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/users.str2int.json\",'r') as f:\n",
    "    users_str2int = json.loads(f.read())\n",
    "\n",
    "dataset = pd.read_csv(data_path+directory+ndataset+\"/ratings.csv\",  ) #dtype = {'rating': np.float, 'itemId': str, 'userId':str}\n",
    "# ratings = SFrame.read_csv(\"/home/kuba/ownCloud/ModGen-fac-mat/sample-data-v2/\"+ndataset+\"/ratings.csv\", column_type_hints=[str,str,float] )\n",
    "with open(data_path+directory+ndataset+\"/items.int2str.json\",'r') as f:\n",
    "    items_int2str = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/items.str2int.json\",'r') as f:\n",
    "    items_str2int = json.loads(f.read())\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset, test_size, relevant):\n",
    "    dataset[\"Testset\"] = False\n",
    "    relevant_ = dataset.loc[dataset['rating']>=relevant]\n",
    "    test_indices = []\n",
    "    for key, user_relevant in relevant_.groupby('userId'):\n",
    "        if(user_relevant.shape[0]>=2):\n",
    "            indeces = user_relevant.index.tolist()\n",
    "            test_indices.extend(np.random.choice(indeces, int(np.ceil(len(indeces)*test_size)), replace = False))\n",
    "\n",
    "\n",
    "    dataset.loc[test_indices, \"Testset\"] = True\n",
    "    print(\"USER DONE\")\n",
    "    testset = dataset[dataset.Testset==True]\n",
    "    trainset = dataset[dataset.Testset==False]\n",
    "\n",
    "    len_m  = 0\n",
    "    for key, trainset_ratings in dataset.groupby('itemId'):\n",
    "        if (np.all(trainset_ratings['Testset'])):\n",
    "\n",
    "            indeces = trainset_ratings.index.tolist()\n",
    "            rand = np.random.choice(indeces, int(np.ceil(len(indeces)*0.1)), replace = False)\n",
    "            len_m +=len(rand)\n",
    "            dataset.loc[rand, 'Testset'] = False\n",
    "\n",
    "    print(\"Move from test set to train set: \", len_m)\n",
    "\n",
    "    return dataset[dataset.Testset==False], dataset[dataset.Testset==True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faktorizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self, trainset, testset, no_fold, relevant = .25, test_size = 0.2, ndataset= None,  users_str2int= None, items_str2int=None):\n",
    "        self.users_str2int, self.items_str2int = users_str2int, items_str2int\n",
    "\n",
    "        #hodnota, kdy je rating oznacen za relevantni\n",
    "        self.relevant = relevant\n",
    "        #velikost trenovaci mnoziny <0,1>\n",
    "        self.test_size = test_size\n",
    "\n",
    "        #slozka vyrazenych uzivatelu\n",
    "        self.no_fold = no_fold\n",
    "\n",
    "\n",
    "        #v init() prirazena matice latentnich vektoru\n",
    "        self.Users = None\n",
    "        self.Items = None\n",
    "\n",
    "        #nazev datasetu, cislo testovaci slozky\n",
    "        self.ndataset, self.no_fold = ndataset, no_fold\n",
    "\n",
    "        self.Trainset = trainset\n",
    "        self.Testset = testset\n",
    "\n",
    "        self.no_ratings = self.Trainset.shape[0]\n",
    "\n",
    "        #mnozina users\n",
    "        self.Users_set = set([str(id_) for id_ in trainset['userId']])\n",
    "        #mnozina items\n",
    "        self.Items_set = set([str(id_) for id_ in trainset['itemId']])\n",
    "\n",
    "        #pocet users\n",
    "        self.no_Users = len(self.Users_set)\n",
    "        #pocet items\n",
    "        self.no_Items = len(self.Items_set)\n",
    "\n",
    "        #mapovaci dictonary, pro snadnejsi praci s poli user_string : uniqe number\n",
    "        self.user_map_dict = dict(zip(self.Users_set,range(self.no_Users)))\n",
    "        self.item_map_dict = dict(zip(self.Items_set,range(self.no_Items)))\n",
    "\n",
    "        #mapovaci lambda funkce list of users : list of mapped numbers\n",
    "        self.u_map = lambda users : list(map(lambda id_: self.user_map_dict[id_], users))\n",
    "        self.i_map = lambda items : list(map(lambda id_: self.item_map_dict[id_], items))\n",
    "\n",
    "        self.Trainset['userId_'] = self.Trainset['userId'].apply(lambda id_: self.user_map_dict[str(id_)])\n",
    "        self.Trainset['itemId_'] = self.Trainset['itemId'].apply(lambda id_: self.item_map_dict[str(id_)])\n",
    "\n",
    "\n",
    "        self.Testset['userId_'] = self.Testset['userId'].apply(lambda id_: self.user_map_dict[str(id_)])\n",
    "        self.Testset['itemId_'] = self.Testset['itemId'].apply(lambda id_: self.item_map_dict[str(id_)])\n",
    "\n",
    "#         self.Item_pop = self.item_pop(self.Trainset)\n",
    "\n",
    "\n",
    "        #TRAINSET se inicializuje az pri spusteni optimalizace\n",
    "\n",
    "\n",
    "\n",
    "        #TESTSET\n",
    "        #dictonary: userId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]}\n",
    "        #ids : indexy items\n",
    "        self.User_Items_testset = self.columns_to_dict(self.Testset, 'userId_', ['itemId_','rating'])\n",
    "        #dictonary: itemId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]}\n",
    "        #ids : indexy users\n",
    "        self.Item_Users_testset = self.columns_to_dict(self.Testset, 'itemId_', ['userId_','rating'])\n",
    "\n",
    "        #koeficient <0,1> ovlivnujici tendenci modelu doporucovat popularni items (1)\n",
    "        self.beta = None\n",
    "\n",
    "\n",
    "    def columns_to_dict(self, df, key_column, value_column):\n",
    "        \"\"\"Transformace pandas.DataFrame do dictonary\"\"\"\n",
    "        dict_ = {}\n",
    "        matrix = df[[key_column] + value_column].as_matrix()\n",
    "\n",
    "        for row in matrix:\n",
    "            key = int(row[0])\n",
    "            ids = int(row[1])\n",
    "            rating = row[2]\n",
    "            if(matrix.shape[1]>3):\n",
    "                weight = row[3]\n",
    "            try:\n",
    "                dict_[key]['ids'].append(ids)\n",
    "                dict_[key]['ratings'].append(rating)\n",
    "                if(matrix.shape[1]>3):\n",
    "                    dict_[key]['weights'].append(weight)\n",
    "\n",
    "            except:\n",
    "                dict_[key] = {'ids':[], 'ratings':[], 'weights':[]}\n",
    "                dict_[key]['ids'].append(ids)\n",
    "                dict_[key]['ratings'].append(rating)\n",
    "                if(matrix.shape[1]>3):\n",
    "                    dict_[key]['weights'].append(weight)\n",
    "\n",
    "\n",
    "        return dict_\n",
    "\n",
    "    def item_pop(self, dataset):\n",
    "        \"\"\"Priradi kazdemu itemu pocet hodnoceni oznacenych jako relevantni (popularita itemu)\n",
    "        dataset : pandas.DataFrame(({userId: string, itemId: string, itemId_ : number, rating: float}))\n",
    "        ----------------------\n",
    "        return dict({item_ : # relevant ratings})\n",
    "        \"\"\"\n",
    "        dict_ = {}\n",
    "\n",
    "        for itemId, dataframe in dataset.groupby('itemId_'):\n",
    "            r_sum = np.sum(dataframe['rating']>=self.relevant)\n",
    "            dict_[itemId] = r_sum\n",
    "\n",
    "        return dict_\n",
    "\n",
    "    def item_weight(self,dataset):\n",
    "        dict_ = {}\n",
    "        for itemId, dataframe in dataset.groupby('itemId_'):\n",
    "            no_relevant_ratings = np.sum(dataframe['rating']>=self.relevant)\n",
    "            dict_[itemId] = pow((1/(no_relevant_ratings+1)),self.beta)\n",
    "\n",
    "        return dict_\n",
    "    '''\n",
    "    INIT\n",
    "    '''\n",
    "    def init(self):\n",
    "        \"\"\" Inicializace matice latentnich vektoru users a items. Inicializace matice U.T*U a V.T*V\"\"\"\n",
    "        if(self.random_init):#Chci nahodne inicializovat latentni vektory pri zapoceti optimalizace s jinymi parametry?\n",
    "            if(self.multiprocessing):#Chci pouzit multiprocessing?\n",
    "                #print(\"START INIT USER AND ITEMS FEATURE VECTORES\")\n",
    "                #Nehrozi paralelni pristup ke sdilenym zdrojum(radkum matice), neni potreba zamykat. Kazdy proces ma urcenou mnozinu radku, ktere updatuje.\n",
    "                user_shared_array = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.random.rand(self.no_Users * self.no_factors), lock=False),dtype=float)\n",
    "                #Matice latentnich vektoru\n",
    "                self.Users = user_shared_array.reshape(self.no_Users, self.no_factors)\n",
    "\n",
    "                item_shared_array = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.random.rand(self.no_Items * self.no_factors), lock= False),dtype=float)\n",
    "                self.Items = item_shared_array.reshape(self.no_Items, self.no_factors)\n",
    "\n",
    "                #Matice sdilena vsemi procesory U.T * U\n",
    "                self.UU = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.zeros(self.no_factors * self.no_factors), lock= False),dtype=float).reshape(self.no_factors, self.no_factors)\n",
    "                self.VV = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.zeros(self.no_factors * self.no_factors), lock= False),dtype=float).reshape(self.no_factors, self.no_factors)\n",
    "                #print(\"FINISH INIT USER AND ITEMS FEATURE VECTORES\")\n",
    "            else:\n",
    "                self.Users = np.random.rand(self.no_Users, self.no_factors)\n",
    "                self.Items = np.random.rand(self.no_Items, self.no_factors)\n",
    "\n",
    "                self.UU = np.random.rand(self.no_factors, self.no_factors)\n",
    "                self.VV = np.random.rand(self.no_factors, self.no_factors)\n",
    "\n",
    "\n",
    "    def init_optimizer(self, beta):\n",
    "        \"\"\"Kontrola nastavenych parametru a prirazeni vah\"\"\"\n",
    "\n",
    "        if(self.weights_mode == \"AllRank\"):\n",
    "            assert beta == 0, \"Beta must be 0! Use AllRank-pop\"\n",
    "\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = 0\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings and \", set(self.Trainset['weight']),\" to observate ratings **\")\n",
    "\n",
    "        elif(self.weights_mode == \"AllRank-pop\"):\n",
    "            print(\"AllRank-pop\")\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = beta\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings\", \", beta: \",self.beta,\" and avg weight \", self.Trainset['weight'].mean(), \"**\")\n",
    "\n",
    "        elif(self.weights_mode == \"MF-RMSE\"):\n",
    "            assert self.weight == 0, \"Weight of missiong values MF-RMSE mode must be 0! Use AllRank or AllRank-pop\"\n",
    "            assert beta == 0, \"Beta must be 0!\"\n",
    "\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = 0\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings **\")\n",
    "\n",
    "        if(self.imputation_value != 0 ):\n",
    "            print(\"** Surrogate missing rating values by imputation value: \", self.imputation_value, \" **\")\n",
    "\n",
    "    def init_users_items_dictonary(self):\n",
    "        self.Trainset['weight'] = self.Trainset['itemId_'].apply(lambda id_: self.Item_weight[id_])\n",
    "        weight_sum = self.Trainset['weight'].sum()\n",
    "        self.Trainset['weight'] = self.Trainset['weight'].apply(lambda w: w/weight_sum*self.no_ratings)\n",
    "\n",
    "        #dictonary: userId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]} ; ids : indexy items\n",
    "        self.User_Items = self.columns_to_dict(self.Trainset, 'userId_', ['itemId_','rating', 'weight'])\n",
    "        #dictonary: itemId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]} ; ids : indexy users\n",
    "        self.Item_Users = self.columns_to_dict(self.Trainset, 'itemId_', ['userId_','rating', 'weight'])\n",
    "\n",
    "\n",
    "    '''\n",
    "    ATOP\n",
    "    '''\n",
    "\n",
    "    def ATOP(self, User_Items):\n",
    "        \"\"\"Vypocet ATOP http://users.cs.fiu.edu/~lzhen001/activities/KDD_USB_key_2010/docs/p713.pdf\"\"\"\n",
    "#         d = now()\n",
    "        pool = multiprocessing.Pool(processes = self.no_processes)\n",
    "        nranks = pool.map_async(NRANKs_u, [(self.Users[user], self.User_Items[user]['ids'], self.Items) for user in self.User_Items.keys()])\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        ATOP = np.mean([item for sublist in nranks.get() for item in sublist])\n",
    "#         print(\"TIME ATOP \", now() - d)\n",
    "        return ATOP\n",
    "\n",
    "\n",
    "    '''\n",
    "    RMSE\n",
    "    '''\n",
    "    def RMSE(self, Item_Users):\n",
    "        \"\"\"Vypocet RMSE\"\"\"\n",
    "#         d = now()\n",
    "        U = self.Users\n",
    "        V = self.Items\n",
    "        errors = []\n",
    "        for item in Item_Users.keys():\n",
    "            ratings = Item_Users[item]['ratings']\n",
    "            users = Item_Users[item]['ids']\n",
    "\n",
    "            users_latent = U[users]\n",
    "            item_latent = V[item].T\n",
    "            errors.extend((np.array(ratings) - (self.imputation_value + np.dot(users_latent, item_latent)))**2)\n",
    "\n",
    "        rmse = sqrt(np.mean(errors))\n",
    "#         print(\"RMSE time \", now() - d)\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    '''\n",
    "    LATENT FACTORS\n",
    "    '''\n",
    "\n",
    "    def items_factor(self,batch):\n",
    "        \"\"\"Update latentnich vektoru. Kazdy procesor dostane disjunktni \"varku\" latentnich vektoru ke zpracovani.\n",
    "        batch: range(i,i+N)\n",
    "\n",
    "        http://users.cs.fiu.edu/~lzhen001/activities/KDD_USB_key_2010/docs/p713.pdf\n",
    "        \"\"\"\n",
    "        V = self.Items\n",
    "        U = self.Users\n",
    "        UU = self.UU\n",
    "        lambda_, r_m  = self.lambda_, self.imputation_value,\n",
    "\n",
    "        for i in batch:\n",
    "            i_rated = self.Item_Users[i]['ids']\n",
    "            U_s = U[i_rated,:]\n",
    "            Wi = np.array([self.Item_Users[i]['weights']])\n",
    "\n",
    "            lM = (np.asmatrix(self.Item_Users[i]['ratings']) - r_m).dot(np.multiply(Wi.T.dot(np.ones((1,self.no_factors))), U_s))\n",
    "            rM = UU - (self.weight*U_s.T).dot(U_s) + np.multiply(U_s.T,  np.ones((self.no_factors,1)).dot(Wi)).dot(U_s)\n",
    "\n",
    "            reg = lambda_ * (self.weight * (self.no_Items-len(Wi)) + (Wi-self.weight).sum()) * np.eye(self.no_factors)\n",
    "            res = np.linalg.solve(rM+reg,lM.T)\n",
    "            #Update latentniho vektoru items matice\n",
    "            V[i,:] = res.flatten()\n",
    "\n",
    "\n",
    "    def users_factor(self, batch):\n",
    "        VV = self.VV\n",
    "        U = self.Users\n",
    "        V = self.Items\n",
    "        lambda_, r_m = self.lambda_, self.imputation_value\n",
    "\n",
    "        for u in batch:\n",
    "            u_rated = self.User_Items[u]['ids']\n",
    "            V_s = V[u_rated,:]\n",
    "            Wu = np.array([self.User_Items[u]['weights']])\n",
    "\n",
    "            lM = (np.asmatrix(self.User_Items[u]['ratings']) - r_m).dot(np.multiply(Wu.T.dot(np.ones((1,self.no_factors))), V_s))\n",
    "            rM = VV - (self.weight*V_s.T).dot(V_s) + np.multiply(V_s.T, np.ones((self.no_factors,1)).dot(Wu)).dot(V_s)\n",
    "\n",
    "            reg = lambda_ * (self.weight * (self.no_Users-len(Wu)) + (Wu-self.weight).sum()) * np.eye(self.no_factors)\n",
    "            res = np.linalg.solve(rM+reg,lM.T)\n",
    "            #Update latentniho vektoru users matice\n",
    "            U[u,:] = res.flatten()\n",
    "\n",
    "    '''\n",
    "    OPTIMIZE RMSE\n",
    "    '''\n",
    "    def optimize_rmse(self, beta):\n",
    "        #inicializuj user/item matici latentnich vektoru a U.T * U, V.T * V (V: matice latentnich vektoru items)\n",
    "        self.init()\n",
    "        self.init_optimizer(beta)\n",
    "\n",
    "        weighted_errors = []\n",
    "        ATOPs = []\n",
    "\n",
    "        step_item = int(np.ceil(len(self.Items)/float(self.no_processes)))\n",
    "        step_user = int(np.ceil(len(self.Users)/float(self.no_processes)))\n",
    "\n",
    "        #Rozdel do disjuktnich, stejne velikych varek\n",
    "        item_range = [range(i,min(i+step_item, self.no_Items)) for i in range(0, self.no_Items, step_item)]\n",
    "        user_range = [range(u,min(u+step_user, self.no_Users)) for u in range(0, self.no_Users, step_user)]\n",
    "\n",
    "        print(\"*******************************\")\n",
    "        print(\"Lambda: \", self.lambda_)\n",
    "        print(\"Impute value: \", self.imputation_value)\n",
    "        print(\"Weight: \", self.weight)\n",
    "        print(\"Beta: \", self.beta)\n",
    "        print(\"Factors: \", self.no_factors)\n",
    "        print(\"Number of processes: \", self.no_processes)\n",
    "        print(\"Number of iterations: \", self.no_iterations)\n",
    "        print(\"*******************************\")\n",
    "\n",
    "#         self.Testset = None\n",
    "#         self.Trainset = None\n",
    "\n",
    "        for ii in range(self.no_iterations):\n",
    "\n",
    "            if(self.multiprocessing):\n",
    "                #ITEMS latent vectors\n",
    "                process = []\n",
    "                self.UU[:] = (self.weight*self.Users.T).dot(self.Users)\n",
    "                d = now()\n",
    "\n",
    "                for batch in item_range:\n",
    "                    p = Process(target = self.items_factor, args = (batch,))\n",
    "                    p.daemon = True\n",
    "                    process.append(p)\n",
    "                    p.start()\n",
    "\n",
    "                [p.join() for p in process]\n",
    "#                 print(\"Item time\",  now()-d)\n",
    "\n",
    "                #USERS latent vectors\n",
    "                process = []\n",
    "                self.VV[:] = (self.weight*self.Items.T).dot(self.Items)\n",
    "                d = now()\n",
    "\n",
    "                for batch in user_range:\n",
    "                    p = Process(target = self.users_factor, args = (batch,))\n",
    "                    p.daemon = True\n",
    "                    process.append(p)\n",
    "                    p.start()\n",
    "\n",
    "                [p.join() for p in process]\n",
    "#                 print(\"User time \",  now()-d)\n",
    "\n",
    "            else:\n",
    "                print(\"Single process\")\n",
    "                self.UU[:] = (self.weight*self.Users.T).dot(self.Users)\n",
    "                for batch in item_range:\n",
    "                    self.items_factor(batch,)\n",
    "                self.VV[:] = (self.weight*self.Items.T).dot(self.Items)\n",
    "                for batch in user_range:\n",
    "                    self.users_factor(batch,)\n",
    "\n",
    "            if(not ii % 1 ):\n",
    "                print(ii,end=\";\")\n",
    "                #vypocti RMSE na training set\n",
    "#                 weighted_errors.append(self.RMSE(self.Item_Users))\n",
    "                #vypocti RMSE na testset\n",
    "                if((self.no_iterations-1) == ii):\n",
    "                    d-now()\n",
    "                    rmse = self.RMSE(self.Item_Users)\n",
    "#                     #vypocti ATOP\n",
    "                    atop = self.ATOP(self.Item_Users)\n",
    "                    ATOPs.append(atop)\n",
    "                    print(\"\\nATOP \", atop, \" RMSE \", rmse, \" TIME \", now()-d)\n",
    "\n",
    "\n",
    "        return weighted_errors, ATOPs\n",
    "\n",
    "\n",
    "    def optimaze(self, no_iterations=1, loss_function=\"RMSE\",  lambda_ = 0.001,  no_processes = 1, no_factors = 50, random_init = True,\n",
    "                 weights_mode = \"MF-RMSE\", weight = 0, imputation_value = 0, beta = 0, mlprocessing= False, save_matrix = False):\n",
    "\n",
    "        if(not mlprocessing):\n",
    "            self.no_processes = 1\n",
    "        else:\n",
    "            self.no_processes = no_processes\n",
    "\n",
    "        self.multiprocessing = mlprocessing\n",
    "        self.loss_function = loss_function\n",
    "        self.lambda_ = lambda_\n",
    "        self.no_factors = no_factors\n",
    "        self.no_iterations = no_iterations\n",
    "        self.random_init = random_init\n",
    "        self.weights_mode = weights_mode\n",
    "        self.weight = weight\n",
    "        self.imputation_value = imputation_value\n",
    "\n",
    "\n",
    "        weighted_errors, ATOPs = self.optimize_rmse(beta)\n",
    "        self.ATOPv = np.max(ATOPs)\n",
    "        if(save_matrix):\n",
    "            self.save_matrices()\n",
    "\n",
    "#         self.plot_rmse(weighted_errors)\n",
    "        return np.max(ATOPs)\n",
    "    '''\n",
    "    PLOT AND EXPLORE\n",
    "    '''\n",
    "    def plot_rmse(self, weighted_errors):\n",
    "        plt.plot(np.log(weighted_errors), label=\"weighted error: \"+str(weighted_errors[-1]))\n",
    "        plt.ylabel(\"RMSE log scale\")\n",
    "        plt.xlabel(\"no iterations\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def explore(self):\n",
    "        print(\"Explore trainset\")\n",
    "        relevant = self.relevant\n",
    "        no_ratings = self.Trainset.shape[0]\n",
    "        no_missing = self.no_Users * self.no_Items - no_ratings\n",
    "        no_all = no_ratings + no_missing\n",
    "\n",
    "        sizes = [no_ratings, no_missing]\n",
    "        labels = [\"ratings\", \"missings\"]\n",
    "        colors = ['yellowgreen', 'lightskyblue']\n",
    "\n",
    "        #pozorovane vs. chybejici hodnoceni\n",
    "        plt.pie(sizes, labels=labels,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90, colors= colors)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        print(\"#{} rating, #{} missing\".format(no_ratings, no_missing))\n",
    "        print(\"Estimate offset w(m): \", no_ratings/no_missing)\n",
    "        ratings = self.Trainset[\"rating\"].values\n",
    "        print(\"Avg of ratings: \", ratings.mean())\n",
    "\n",
    "        #relevantni vs. irelevantni hodnoceni\n",
    "        no_relevant = np.sum(ratings>=relevant)\n",
    "        no_irelevant = np.sum(ratings<relevant)\n",
    "        labels = [\"relevant\", \"irrelevenat\"]\n",
    "\n",
    "        sizes = [no_relevant, no_irelevant]\n",
    "        colors = ['lightskyblue', 'lightcoral']\n",
    "        plt.pie(sizes, labels=labels,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90, colors= colors)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"#{} relevant, #{} irelevant\".format(no_relevant, no_irelevant))\n",
    "\n",
    "        #rozlozeni popularity mezi items\n",
    "        items_popularity = np.sort(list(self.Item_pop.values()))[::-1]\n",
    "#         plt.bar(range(len(items_popularity)),items_popularity)\n",
    "        plt.xticks([])\n",
    "        plt.ylabel(\"# of item rating marked as relevant\")\n",
    "        plt.xlabel(\"item\")\n",
    "        plt.plot(items_popularity,'_')\n",
    "        plt.show()\n",
    "\n",
    "        #Histogram hodnoceni\n",
    "        plt.hist(self.Trainset['rating'].values,bins = len(set(self.Trainset['rating'])))\n",
    "        plt.xlabel(\"rating\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_matrices(self):\n",
    "        d = now()\n",
    "        no_iterations, no_factors, lambda_ , weight, r_m = self.no_iterations, self.no_factors, self.lambda_, self.weight, self.imputation_value\n",
    "        ndataset, fold = self.ndataset, self.no_fold\n",
    "        if not os.path.exists(data_path+\"MATRICES/\"+ndataset):\n",
    "            os.makedirs(data_path+\"MATRICES/\"+ndataset)\n",
    "\n",
    "        with open(data_path+\"MATRICES/\"+ndataset+\"/\"+ndataset+str(no_fold)+\"_model_f:\"+str(no_factors)+\"l:\"+str(lambda_)+\"w:\"+str(weight)+\"b:\"+str(self.beta)+\"r:\"+str(self.imputation_value)+\"_ATOP:\"+str(self.ATOPv)+\".txt\", 'w') as f: #+\"w\"+str(weight)+\"r\"+str(r_m)+\n",
    "            f.write(\"m \"+str(self.Users.shape[0])+\"\\n\")\n",
    "            f.write(\"n \"+str(self.Items.shape[0])+\"\\n\")\n",
    "            f.write(\"k \"+str(no_factors)+\"\\n\")\n",
    "\n",
    "            user_map = {v: k for k, v in self.user_map_dict.items()}\n",
    "            for idx, laten_vec in enumerate(self.Users):\n",
    "                string_idx = user_map[idx]\n",
    "                idf = self.users_str2int[string_idx]\n",
    "                f.write(\"p\"+str(idf)+\" \"+' '.join(list(laten_vec.astype('str')))+\"\\n\")\n",
    "\n",
    "            item_map = {v: k for k, v in self.item_map_dict.items()}\n",
    "            for idx, laten_vec in enumerate(self.Items):\n",
    "                string_idx = item_map[idx]\n",
    "                idf = self.items_str2int[string_idx]\n",
    "                f.write(\"q\"+str(idf)+\" \"+' '.join(list(laten_vec.astype('str')))+\"\\n\")\n",
    "\n",
    "        #print(\"************ UKLADANI MATICE \", now()-d)\n",
    "\n",
    "def NRANKs_u(args):\n",
    "    \"\"\"Vypocti normalizovany rank pro uzivatele.\n",
    "    args: user latentni vektor, user testovaci items, matice latentnich vektoru items\n",
    "    \"\"\"\n",
    "    user_vec, items, V = args\n",
    "\n",
    "    ratings_hat = np.dot(user_vec, V.T)\n",
    "    N = ratings_hat.shape[0]\n",
    "    ratings = ratings_hat[items]\n",
    "    nranks = np.array(list(map(lambda rating: np.sum(ratings_hat<rating), ratings)))/N\n",
    "\n",
    "    return nranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER DONE\n",
      "Move from test set to train set:  0\n",
      "AllRank-pop\n",
      "** Set weight:  0.02  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0\n",
      "Weight:  0.02\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;4;5;\n",
      "ATOP  0.904865748381  RMSE  0.14498367919250435  TIME  0:00:01.405750\n",
      "********** TIME  5 0:00:12.482459\n",
      "************************************\n",
      "AllRank-pop\n",
      "** Set weight:  0.02  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "** Surrogate missing rating values by imputation value:  0.01  **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0.01\n",
      "Weight:  0.02\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;4;5;\n",
      "ATOP  0.889348132746  RMSE  0.1432898185357616  TIME  0:00:01.289597\n",
      "********** TIME  5 0:00:06.183513\n",
      "************************************\n",
      "AllRank-pop\n",
      "** Set weight:  0.05  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0\n",
      "Weight:  0.05\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;4;5;\n",
      "ATOP  0.917908732097  RMSE  0.1744601154291158  TIME  0:00:01.388926\n",
      "********** TIME  5 0:00:06.236371\n",
      "************************************\n",
      "AllRank-pop\n",
      "** Set weight:  0.05  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "** Surrogate missing rating values by imputation value:  0.01  **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0.01\n",
      "Weight:  0.05\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;4;5;\n",
      "ATOP  0.90500722752  RMSE  0.17194553110795668  TIME  0:00:01.400019\n",
      "********** TIME  5 0:00:06.240047\n",
      "************************************\n",
      "AllRank-pop\n",
      "** Set weight:  0.08  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0\n",
      "Weight:  0.08\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;4;5;\n",
      "ATOP  0.919185255397  RMSE  0.18910689972049496  TIME  0:00:01.408776\n",
      "********** TIME  5 0:00:06.253667\n",
      "************************************\n",
      "AllRank-pop\n",
      "** Set weight:  0.08  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "** Surrogate missing rating values by imputation value:  0.01  **\n",
      "*******************************\n",
      "Lambda:  0\n",
      "Impute value:  0.01\n",
      "Weight:  0.08\n",
      "Beta:  0\n",
      "Factors:  10\n",
      "Number of processes:  5\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "0;1;2;3;"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-368:\n",
      "Traceback (most recent call last):\n",
      "Process Process-367:\n",
      "Process Process-369:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-21af7968d267>\", line 249, in items_factor\n",
      "    lM = (np.asmatrix(self.Item_Users[i]['ratings']) - r_m).dot(np.multiply(Wi.T.dot(np.ones((1,self.no_factors))), U_s))\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\", line 98, in asmatrix\n",
      "    return matrix(data, dtype=dtype, copy=False)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\", line 292, in __array_finalize__\n",
      "    def __array_finalize__(self, obj):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-3-21af7968d267>\", line 253, in items_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 384, in solve\n",
      "    r = gufunc(a, b, signature=signature, extobj=extobj)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\", line 289, in __new__\n",
      "    order=order)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-21af7968d267>\", line 252, in items_factor\n",
      "    reg = lambda_ * (self.weight * (self.no_Items-len(Wi)) + (Wi-self.weight).sum()) * np.eye(self.no_factors)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/core/_methods.py\", line 32, in _sum\n",
      "    return umr_sum(a, axis, dtype, out, keepdims)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process Process-366:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-21af7968d267>\", line 249, in items_factor\n",
      "    lM = (np.asmatrix(self.Item_Users[i]['ratings']) - r_m).dot(np.multiply(Wi.T.dot(np.ones((1,self.no_factors))), U_s))\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/matrixlib/defmatrix.py\", line 294, in __array_finalize__\n",
      "    if (isinstance(obj, matrix) and obj._getitem): return\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-dbd1635850bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                                     ATOP = MFact.optimaze(no_iterations = no_iterations,  lambda_ = lambda_, no_processes = p, no_factors = no_factor,\n\u001b[0;32m     24\u001b[0m                                                           \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                                                           weights_mode = \"AllRank-pop\", weight = weight, imputation_value = imputation_value , random_init = True, mlprocessing = True, save_matrix = SAVE_MATRIX)\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"********** TIME \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-21af7968d267>\u001b[0m in \u001b[0;36moptimaze\u001b[1;34m(self, no_iterations, loss_function, lambda_, no_processes, no_factors, random_init, weights_mode, weight, imputation_value, beta, mlprocessing, save_matrix)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mweighted_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mATOPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_rmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mATOPv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mATOPs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-21af7968d267>\u001b[0m in \u001b[0;36moptimize_rmse\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m    320\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;31m#                 print(\"Item time\",  now()-d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-21af7968d267>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    320\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m \u001b[1;31m#                 print(\"Item time\",  now()-d)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a child process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a started process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                     \u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[1;31m# Child process not yet created. See #1731717\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    test_size, relevant = 0.0, 1\n",
    "    SAVE_MATRIX = True\n",
    "    #[0,1,2,3,4,5,6,7,8,9]\n",
    "    for no_fold in [0,1,2,3,4,5,6,7,8,9]:  #iteruj pres testovaci slozky\n",
    "        #odstran z datasetu testovaci users obsazene ve slozce\n",
    "        ratings = dataset[~dataset.userId.isin(user_folds[no_fold])]\n",
    "        trainset, testset = split_dataset(ratings.copy(),test_size = test_size, relevant = relevant)\n",
    "\n",
    "        MFact = MatrixFactorization(trainset, testset, no_fold = no_fold , test_size = test_size, relevant = relevant, ndataset = ndataset,\n",
    "                                    users_str2int= users_str2int, items_str2int = items_str2int)\n",
    "        #[0, 0.0008, 0.001, 0.0015 ,0.002, 0.005, 0.01, 0.02]\n",
    "        for lambda_ in [0, 0.001, 0.005, 0.008, 0.01]: #iteruj pres lambda\n",
    "            #[1, 2, 5, 10, 30, 50, 100,200, 300],\n",
    "            for no_factor in [10,30,100]: # iteruj pres delku latentnich vektoru\n",
    "                for no_iterations in [6]: #iteruj pres pocet iteraci alternating least square\n",
    "                    #[-2,-1,-0.5,-0.2,0, 0.2, 0.5, 1, 2]\n",
    "                    for beta in [0, 0.1, 0.2]:\n",
    "                        for weight in [0.02, 0.05, 0.08, 0.1]:\n",
    "                            for imputation_value in [0, 0.01]:\n",
    "                                for p in [5]:\n",
    "                                    d = now()\n",
    "                                    ATOP = MFact.optimaze(no_iterations = no_iterations,  lambda_ = lambda_, no_processes = p, no_factors = no_factor,\n",
    "                                                          beta = beta,\n",
    "                                                          weights_mode = \"AllRank-pop\", weight = weight, imputation_value = imputation_value , random_init = True, mlprocessing = True, save_matrix = SAVE_MATRIX)\n",
    "\n",
    "                                    print(\"********** TIME \",p, now() - d)\n",
    "                                    print(\"************************************\")\n",
    "#                     print(\"ATOP \", ATOP)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "line profiler\n",
    "\"\"\"\n",
    "# user = max_key\n",
    "# %load_ext line_profiler\n",
    "# %lprun -f NRANKs_u NRANKs_u((MFact.Users[user], MFact.User_Items_testset[user]['ids'], MFact.Items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
