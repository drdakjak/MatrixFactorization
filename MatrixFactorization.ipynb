{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from IPython.core.display import display\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from multiprocessing import Process, Pool\n",
    "import multiprocessing\n",
    "from scipy.linalg import solve\n",
    "\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "import ctypes\n",
    "import datetime\n",
    "from math import sqrt\n",
    "import platform\n",
    "platform.architecture()\n",
    "# %load_ext line_profiler\n",
    "import scipy\n",
    "import os\n",
    "# %load_ext Cython\n",
    "now = lambda: datetime.datetime.now()\n",
    "# %load_ext line_profiler\n",
    "import sys\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nacteni dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuba/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "directory = 'sample-data-v2/'\n",
    "ndataset = 'goout'#\"movielens_1m\"\n",
    "data_path = '/home/kuba/ownCloud/Recombee/'\n",
    "\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/items.json\",'r') as f:\n",
    "    items = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/properties.json\",'r') as f:\n",
    "    properties = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/user.folds.json\",'r') as f:\n",
    "    user_folds = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/users.int2str.json\",'r') as f:\n",
    "    users_int2str = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/users.str2int.json\",'r') as f:\n",
    "    users_str2int = json.loads(f.read())\n",
    "\n",
    "dataset = pd.read_csv(data_path+directory+ndataset+\"/ratings.csv\",  ) #dtype = {'rating': np.float, 'itemId': str, 'userId':str}\n",
    "# ratings = SFrame.read_csv(\"/home/kuba/ownCloud/ModGen-fac-mat/sample-data-v2/\"+ndataset+\"/ratings.csv\", column_type_hints=[str,str,float] )\n",
    "with open(data_path+directory+ndataset+\"/items.int2str.json\",'r') as f:\n",
    "    items_int2str = json.loads(f.read())\n",
    "\n",
    "with open(data_path+directory+ndataset+\"/items.str2int.json\",'r') as f:\n",
    "    items_str2int = json.loads(f.read())\n",
    "\n",
    "\n",
    "\n",
    "def split_dataset(dataset, test_size, relevant):        \n",
    "    dataset[\"Testset\"] = False\n",
    "    if(test_size == 0):\n",
    "        return dataset[dataset.Testset==False], dataset[dataset.Testset==True] \n",
    "    \n",
    "    relevant_ = dataset.loc[dataset['rating']>=relevant]\n",
    "    test_indices = []\n",
    "    for key, user_relevant in relevant_.groupby('userId'):\n",
    "        if(user_relevant.shape[0]>=2):\n",
    "            indeces = user_relevant.index.tolist()\n",
    "            test_indices.extend(np.random.choice(indeces, int(np.ceil(len(indeces)*test_size)), replace = False))\n",
    "\n",
    "\n",
    "    dataset.loc[test_indices, \"Testset\"] = True\n",
    "    print(\"USER DONE\")\n",
    "    testset = dataset[dataset.Testset==True]\n",
    "    trainset = dataset[dataset.Testset==False]\n",
    "\n",
    "    len_m  = 0\n",
    "    for key, trainset_ratings in dataset.groupby('itemId'):\n",
    "        if (np.all(trainset_ratings['Testset'])):\n",
    "\n",
    "            indeces = trainset_ratings.index.tolist()\n",
    "            rand = np.random.choice(indeces, int(np.ceil(len(indeces)*0.1)), replace = False)\n",
    "            len_m +=len(rand)\n",
    "            dataset.loc[rand, 'Testset'] = False\n",
    "\n",
    "    print(\"Move from test set to train set: \", len_m)\n",
    "\n",
    "    return dataset[dataset.Testset==False], dataset[dataset.Testset==True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faktorizace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization:\n",
    "    def __init__(self, trainset, testset, no_fold, relevant = .25, test_size = 0.2, ndataset= None,  users_str2int= None, items_str2int=None):\n",
    "        self.users_str2int, self.items_str2int = users_str2int, items_str2int\n",
    "\n",
    "        #hodnota, kdy je rating oznacen za relevantni\n",
    "        self.relevant = relevant\n",
    "        #velikost trenovaci mnoziny <0,1>\n",
    "        self.test_size = test_size\n",
    "\n",
    "        #slozka vyrazenych uzivatelu\n",
    "        self.no_fold = no_fold\n",
    "\n",
    "\n",
    "        #v init() prirazena matice latentnich vektoru\n",
    "        self.Users = None\n",
    "        self.Items = None\n",
    "\n",
    "        #nazev datasetu, cislo testovaci slozky\n",
    "        self.ndataset, self.no_fold = ndataset, no_fold\n",
    "\n",
    "        self.Trainset = trainset\n",
    "        self.Testset = testset\n",
    "\n",
    "        self.no_ratings = self.Trainset.shape[0]\n",
    "\n",
    "        #mnozina users\n",
    "        self.Users_set = set([str(id_) for id_ in trainset['userId']])\n",
    "        #mnozina items\n",
    "        self.Items_set = set([str(id_) for id_ in trainset['itemId']])\n",
    "\n",
    "        #pocet users\n",
    "        self.no_Users = len(self.Users_set)\n",
    "        #pocet items\n",
    "        self.no_Items = len(self.Items_set)\n",
    "\n",
    "        #mapovaci dictonary, pro snadnejsi praci s poli user_string : uniqe number\n",
    "        self.user_map_dict = dict(zip(self.Users_set,range(self.no_Users)))\n",
    "        self.item_map_dict = dict(zip(self.Items_set,range(self.no_Items)))\n",
    "\n",
    "        #mapovaci lambda funkce list of users : list of mapped numbers\n",
    "        self.u_map = lambda users : list(map(lambda id_: self.user_map_dict[id_], users))\n",
    "        self.i_map = lambda items : list(map(lambda id_: self.item_map_dict[id_], items))\n",
    "\n",
    "        self.Trainset['userId_'] = self.Trainset['userId'].apply(lambda id_: self.user_map_dict[str(id_)])\n",
    "        self.Trainset['itemId_'] = self.Trainset['itemId'].apply(lambda id_: self.item_map_dict[str(id_)])\n",
    "\n",
    "\n",
    "        self.Testset['userId_'] = self.Testset['userId'].apply(lambda id_: self.user_map_dict[str(id_)])\n",
    "        self.Testset['itemId_'] = self.Testset['itemId'].apply(lambda id_: self.item_map_dict[str(id_)])\n",
    "\n",
    "#         self.Item_pop = self.item_pop(self.Trainset)\n",
    "\n",
    "\n",
    "        #TRAINSET se inicializuje az pri spusteni optimalizace\n",
    "\n",
    "\n",
    "\n",
    "        #TESTSET\n",
    "        #dictonary: userId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]}\n",
    "        #ids : indexy items\n",
    "        self.User_Items_testset = self.columns_to_dict(self.Testset, 'userId_', ['itemId_','rating'])\n",
    "        #dictonary: itemId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]}\n",
    "        #ids : indexy users\n",
    "        self.Item_Users_testset = self.columns_to_dict(self.Testset, 'itemId_', ['userId_','rating'])\n",
    "\n",
    "        #koeficient <0,1> ovlivnujici tendenci modelu doporucovat popularni items (1)\n",
    "        self.beta = None\n",
    "\n",
    "\n",
    "    def columns_to_dict(self, df, key_column, value_column):\n",
    "        \"\"\"Transformace pandas.DataFrame do dictonary\"\"\"\n",
    "        dict_ = {}\n",
    "        matrix = df[[key_column] + value_column].as_matrix()\n",
    "\n",
    "        for row in matrix:\n",
    "            key = int(row[0])\n",
    "            ids = int(row[1])\n",
    "            rating = row[2]\n",
    "            if(matrix.shape[1]>3):\n",
    "                weight = row[3]\n",
    "            try:\n",
    "                dict_[key]['ids'].append(ids)\n",
    "                dict_[key]['ratings'].append(rating)\n",
    "                if(matrix.shape[1]>3):\n",
    "                    dict_[key]['weights'].append(weight)\n",
    "\n",
    "            except:\n",
    "                dict_[key] = {'ids':[], 'ratings':[], 'weights':[]}\n",
    "                dict_[key]['ids'].append(ids)\n",
    "                dict_[key]['ratings'].append(rating)\n",
    "                if(matrix.shape[1]>3):\n",
    "                    dict_[key]['weights'].append(weight)\n",
    "\n",
    "\n",
    "        return dict_\n",
    "\n",
    "    def item_pop(self, dataset):\n",
    "        \"\"\"Priradi kazdemu itemu pocet hodnoceni oznacenych jako relevantni (popularita itemu)\n",
    "        dataset : pandas.DataFrame(({userId: string, itemId: string, itemId_ : number, rating: float}))\n",
    "        ----------------------\n",
    "        return dict({item_ : # relevant ratings})\n",
    "        \"\"\"\n",
    "        dict_ = {}\n",
    "\n",
    "        for itemId, dataframe in dataset.groupby('itemId_'):\n",
    "            r_sum = np.sum(dataframe['rating']>=self.relevant)\n",
    "            dict_[itemId] = r_sum\n",
    "\n",
    "        return dict_\n",
    "\n",
    "    def item_weight(self,dataset):\n",
    "        dict_ = {}\n",
    "        for itemId, dataframe in dataset.groupby('itemId_'):\n",
    "            no_relevant_ratings = np.sum(dataframe['rating']>=self.relevant)\n",
    "            dict_[itemId] = pow((1/(no_relevant_ratings+1)),self.beta)\n",
    "\n",
    "        return dict_\n",
    "    '''\n",
    "    INIT\n",
    "    '''\n",
    "    def init(self):\n",
    "        \"\"\" Inicializace matice latentnich vektoru users a items. Inicializace matice U.T*U a V.T*V\"\"\"\n",
    "        if(self.random_init):#Chci nahodne inicializovat latentni vektory pri zapoceti optimalizace s jinymi parametry?\n",
    "            if(self.multiprocessing):#Chci pouzit multiprocessing?\n",
    "                #print(\"START INIT USER AND ITEMS FEATURE VECTORES\")\n",
    "                #Nehrozi paralelni pristup ke sdilenym zdrojum(radkum matice), neni potreba zamykat. Kazdy proces ma urcenou mnozinu radku, ktere updatuje.\n",
    "                user_shared_array = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.random.rand(self.no_Users * self.no_factors), lock=False),dtype=float)\n",
    "                #Matice latentnich vektoru\n",
    "                self.Users = user_shared_array.reshape(self.no_Users, self.no_factors)\n",
    "\n",
    "                item_shared_array = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.random.rand(self.no_Items * self.no_factors), lock= False),dtype=float)\n",
    "                self.Items = item_shared_array.reshape(self.no_Items, self.no_factors)\n",
    "\n",
    "                #Matice sdilena vsemi procesory U.T * U\n",
    "                self.UU = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.zeros(self.no_factors * self.no_factors), lock= False),dtype=float).reshape(self.no_factors, self.no_factors)\n",
    "                self.VV = np.frombuffer(multiprocessing.Array(ctypes.c_double, np.zeros(self.no_factors * self.no_factors), lock= False),dtype=float).reshape(self.no_factors, self.no_factors)\n",
    "                #print(\"FINISH INIT USER AND ITEMS FEATURE VECTORES\")\n",
    "            else:\n",
    "                self.Users = np.random.rand(self.no_Users, self.no_factors)\n",
    "                self.Items = np.random.rand(self.no_Items, self.no_factors)\n",
    "\n",
    "                self.UU = np.random.rand(self.no_factors, self.no_factors)\n",
    "                self.VV = np.random.rand(self.no_factors, self.no_factors)\n",
    "\n",
    "\n",
    "    def init_optimizer(self, beta):\n",
    "        \"\"\"Kontrola nastavenych parametru a prirazeni vah\"\"\"\n",
    "\n",
    "        if(self.weights_mode == \"AllRank\"):\n",
    "            assert beta == 0, \"Beta must be 0! Use AllRank-pop\"\n",
    "\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = 0\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings and \", set(self.Trainset['weight']),\" to observate ratings **\")\n",
    "\n",
    "        elif(self.weights_mode == \"AllRank-pop\"):\n",
    "            print(\"AllRank-pop\")\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = beta\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings\", \", beta: \",self.beta,\" and avg weight \", self.Trainset['weight'].mean(), \"**\")\n",
    "\n",
    "        elif(self.weights_mode == \"MF-RMSE\"):\n",
    "            assert self.weight == 0, \"Weight of missiong values MF-RMSE mode must be 0! Use AllRank or AllRank-pop\"\n",
    "            assert beta == 0, \"Beta must be 0!\"\n",
    "\n",
    "            if(self.beta is None or self.beta != beta):\n",
    "                self.beta = 0\n",
    "                self.Item_weight = self.item_weight(self.Trainset)\n",
    "                self.init_users_items_dictonary()\n",
    "            print(\"** Set weight: \",self.weight, \" to missing ratings **\")\n",
    "\n",
    "        if(self.imputation_value != 0 ):\n",
    "            print(\"** Surrogate missing rating values by imputation value: \", self.imputation_value, \" **\")\n",
    "\n",
    "    def init_users_items_dictonary(self):\n",
    "        self.Trainset['weight'] = self.Trainset['itemId_'].apply(lambda id_: self.Item_weight[id_])\n",
    "        weight_sum = self.Trainset['weight'].sum()\n",
    "        self.Trainset['weight'] = self.Trainset['weight'].apply(lambda w: w/weight_sum*self.no_ratings)\n",
    "\n",
    "        #dictonary: userId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]} ; ids : indexy items\n",
    "        self.User_Items = self.columns_to_dict(self.Trainset, 'userId_', ['itemId_','rating', 'weight'])\n",
    "        #dictonary: itemId : {\"ids\":[...], \"ratings\":[...], 'weights':[...]} ; ids : indexy users\n",
    "        self.Item_Users = self.columns_to_dict(self.Trainset, 'itemId_', ['userId_','rating', 'weight'])\n",
    "\n",
    "\n",
    "    '''\n",
    "    ATOP\n",
    "    '''\n",
    "\n",
    "    def ATOP(self, User_Items):\n",
    "        \"\"\"Vypocet ATOP http://users.cs.fiu.edu/~lzhen001/activities/KDD_USB_key_2010/docs/p713.pdf\"\"\"\n",
    "#         d = now()\n",
    "        pool = multiprocessing.Pool(processes = self.no_processes)\n",
    "        nranks = pool.map_async(NRANKs_u, [(self.Users[user], self.User_Items[user]['ids'], self.Items) for user in self.User_Items.keys()])\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        ATOP = np.mean([item for sublist in nranks.get() for item in sublist])\n",
    "#         print(\"TIME ATOP \", now() - d)\n",
    "        return ATOP\n",
    "\n",
    "\n",
    "    '''\n",
    "    RMSE\n",
    "    '''\n",
    "    def RMSE(self, Item_Users):\n",
    "        \"\"\"Vypocet RMSE\"\"\"\n",
    "#         d = now()\n",
    "        U = self.Users\n",
    "        V = self.Items\n",
    "        errors = []\n",
    "        for item in Item_Users.keys():\n",
    "            ratings = Item_Users[item]['ratings']\n",
    "            users = Item_Users[item]['ids']\n",
    "\n",
    "            users_latent = U[users]\n",
    "            item_latent = V[item].T\n",
    "            errors.extend((np.array(ratings) - (self.imputation_value + np.dot(users_latent, item_latent)))**2)\n",
    "\n",
    "        rmse = sqrt(np.mean(errors))\n",
    "#         print(\"RMSE time \", now() - d)\n",
    "        return rmse\n",
    "\n",
    "\n",
    "    '''\n",
    "    LATENT FACTORS\n",
    "    '''\n",
    "\n",
    "    def items_factor(self,batch):\n",
    "        \"\"\"Update latentnich vektoru. Kazdy procesor dostane disjunktni \"varku\" latentnich vektoru ke zpracovani.\n",
    "        batch: range(i,i+N)\n",
    "\n",
    "        http://users.cs.fiu.edu/~lzhen001/activities/KDD_USB_key_2010/docs/p713.pdf\n",
    "        \"\"\"\n",
    "        V = self.Items\n",
    "        U = self.Users\n",
    "        UU = self.UU\n",
    "        lambda_, r_m  = self.lambda_, self.imputation_value,\n",
    "\n",
    "        weight, no_factors,no_Items = self.weight, self.no_factors, self.no_Items\n",
    "        eye = np.eye(no_factors)\n",
    "        d = now()\n",
    "        for i in batch:\n",
    "            item_users = self.Item_Users[i]\n",
    "            i_rated = item_users['ids']\n",
    "            U_s = np.take(U, i_rated, axis=0)\n",
    "            \n",
    "            Wi = np.array([item_users['weights']])\n",
    "\n",
    "            lM = (np.array([item_users['ratings']]) - r_m).dot(np.multiply(Wi.T.dot(np.ones((1,no_factors))), U_s))\n",
    "            rM = UU - (weight*U_s.T).dot(U_s) + np.multiply(U_s.T,  np.ones((no_factors,1)).dot(Wi)).dot(U_s)\n",
    "            reg = lambda_ * (weight * (no_Items-len(Wi)) + (Wi-weight).sum()) * eye\n",
    "            res = np.linalg.solve(rM+reg,lM.T)\n",
    "            #Update latentniho vektoru items matice\n",
    "            V[i,:] = res.ravel()\n",
    "        print(\"ITEM TIME \", now() - d)\n",
    "\n",
    "\n",
    "    def users_factor(self, batch):\n",
    "        VV = self.VV\n",
    "        U = self.Users\n",
    "        V = self.Items\n",
    "        lambda_, r_m = self.lambda_, self.imputation_value\n",
    "\n",
    "        weight, no_factors,no_Users = self.weight, self.no_factors, self.no_Users\n",
    "        eye = np.eye(no_factors)\n",
    "        d = now()\n",
    "        for u in batch:\n",
    "            user_items = self.User_Items[u]\n",
    "            u_rated = user_items['ids']\n",
    "            \n",
    "            V_s = np.take(V, u_rated, axis=0)\n",
    "            \n",
    "            Wu = np.array([user_items['weights']])\n",
    "\n",
    "            lM = (np.array([user_items['ratings']]) - r_m).dot(np.multiply(Wu.T.dot(np.ones((1,no_factors))), V_s))\n",
    "            rM = VV - (weight*V_s.T).dot(V_s) + np.multiply(V_s.T, np.ones((no_factors,1)).dot(Wu)).dot(V_s)\n",
    "            reg = lambda_ * (weight * (no_Users-Wu.shape[0]) + (Wu-weight).sum()) * eye\n",
    "            res = np.linalg.solve(rM+reg,lM.T)\n",
    "            #Update latentniho vektoru users matice\n",
    "            U[u,:] = res.ravel()\n",
    "    \n",
    "        \n",
    "\n",
    "    def users_factor1(self, batch):\n",
    "        VV = self.VV\n",
    "        U = self.Users\n",
    "        V = self.Items\n",
    "        lambda_, r_m = self.lambda_, self.imputation_value\n",
    "\n",
    "        weight, no_factors,no_Users = self.weight, self.no_factors, self.no_Users\n",
    "        eye = np.eye(no_factors)\n",
    "        d = now()\n",
    "        for u in batch:\n",
    "            u_rated = self.User_Items[u]['ids']\n",
    "            \n",
    "            V_s = V[u_rated,:]\n",
    "            \n",
    "            Wu = np.array([self.User_Items[u]['weights']])\n",
    "\n",
    "            lM = (np.asmatrix(self.User_Items[u]['ratings']) - r_m).dot(np.multiply(Wu.T.dot(np.ones((1,no_factors))), V_s))\n",
    "            rM = VV - (weight*V_s.T).dot(V_s) + np.multiply(V_s.T, np.ones((no_factors,1)).dot(Wu)).dot(V_s)\n",
    "            reg = lambda_ * (weight * (no_Users-Wu.shape[0]) + (Wu-weight).sum()) * eye\n",
    "            res = np.linalg.solve(rM+reg,lM.T)\n",
    "            #Update latentniho vektoru users matice\n",
    "            U[u,:] = res.flatten()\n",
    "    '''\n",
    "    OPTIMIZE RMSE\n",
    "    '''\n",
    "    def optimize_rmse(self, beta):\n",
    "        #inicializuj user/item matici latentnich vektoru a U.T * U, V.T * V (V: matice latentnich vektoru items)\n",
    "        self.init()\n",
    "        self.init_optimizer(beta)\n",
    "\n",
    "        weighted_errors = []\n",
    "        ATOPs = []\n",
    "\n",
    "        step_item = int(np.ceil(len(self.Items)/float(self.no_processes)))\n",
    "        step_user = int(np.ceil(len(self.Users)/float(self.no_processes)))\n",
    "\n",
    "        #Rozdel do disjuktnich, stejne velikych varek\n",
    "        item_range = [range(i,min(i+step_item, self.no_Items)) for i in range(0, self.no_Items, step_item)]\n",
    "        user_range = [range(u,min(u+step_user, self.no_Users)) for u in range(0, self.no_Users, step_user)]\n",
    "\n",
    "        print(\"*******************************\")\n",
    "        print(\"Lambda: \", self.lambda_)\n",
    "        print(\"Impute value: \", self.imputation_value)\n",
    "        print(\"Weight: \", self.weight)\n",
    "        print(\"Beta: \", self.beta)\n",
    "        print(\"Factors: \", self.no_factors)\n",
    "        print(\"Number of processes: \", self.no_processes)\n",
    "        print(\"Number of iterations: \", self.no_iterations)\n",
    "        print(\"*******************************\")\n",
    "\n",
    "#         self.Testset = None\n",
    "#         self.Trainset = None\n",
    "\n",
    "        for ii in range(self.no_iterations):\n",
    "\n",
    "            if(self.multiprocessing):\n",
    "                #ITEMS latent vectors\n",
    "                process = []\n",
    "\n",
    "                d = now()\n",
    "                self.UU[:] = (self.weight*self.Users.T).dot(self.Users)\n",
    "                print(\"UU \", now() - d)\n",
    "\n",
    "                d = now()\n",
    "                for batch in item_range:\n",
    "                    p = Process(target = self.items_factor, args = (batch,))\n",
    "                    p.daemon = True\n",
    "                    process.append(p)\n",
    "                    p.start()\n",
    "\n",
    "                [p.join() for p in process]\n",
    "                print(\"Item time\",  now()-d)\n",
    "\n",
    "                #USERS latent vectors\n",
    "                process = []\n",
    "\n",
    "                d = now()\n",
    "                self.VV[:] = (self.weight*self.Items.T).dot(self.Items)\n",
    "                print(\"VV \", now() - d)\n",
    "\n",
    "                d = now()\n",
    "                for batch in user_range:\n",
    "                    p = Process(target = self.users_factor, args = (batch,))\n",
    "                    p.daemon = True\n",
    "                    process.append(p)\n",
    "                    p.start()\n",
    "\n",
    "                [p.join() for p in process]\n",
    "                print(\"User time \",  now()-d)\n",
    "\n",
    "            else:\n",
    "                print(\"Single process\")\n",
    "                self.UU[:] = (self.weight*self.Users.T).dot(self.Users)\n",
    "                for batch in item_range:\n",
    "                    self.items_factor(batch,)\n",
    "                self.VV[:] = (self.weight*self.Items.T).dot(self.Items)\n",
    "                for batch in user_range:\n",
    "                    self.users_factor(batch,)\n",
    "\n",
    "\n",
    "            print(ii,end=\";\")\n",
    "            #vypocti RMSE na training set\n",
    "#                 weighted_errors.append(self.RMSE(self.Item_Users))\n",
    "            #vypocti RMSE na testset\n",
    "            if((self.no_iterations-1) == ii):\n",
    "                d = now()\n",
    "                rmse = self.RMSE(self.Item_Users)\n",
    "                #vypocti ATOP\n",
    "                if(self.compute_ATOP):\n",
    "                    atop = self.ATOP(self.Item_Users)\n",
    "                    ATOPs.append(atop)\n",
    "                    print(\"\\nATOP \", atop, \" RMSE \", rmse, \" TIME \", now()-d)\n",
    "                else:\n",
    "                    ATOPs = [0]\n",
    "                    print(\"\\nRMSE\", rmse, \" TIME \", now()-d)\n",
    "\n",
    "        return weighted_errors, ATOPs\n",
    "\n",
    "\n",
    "    def optimaze(self, no_iterations=1, loss_function=\"RMSE\",  lambda_ = 0.001,  no_processes = 1, no_factors = 50, random_init = True,\n",
    "                 weights_mode = \"MF-RMSE\", weight = 0, imputation_value = 0, beta = 0, mlprocessing= False, save_matrix = False, compute_ATOP = False):\n",
    "\n",
    "        if(not mlprocessing):\n",
    "            self.no_processes = 1\n",
    "        else:\n",
    "            self.no_processes = no_processes\n",
    "\n",
    "        self.multiprocessing = mlprocessing\n",
    "        self.loss_function = loss_function\n",
    "        self.lambda_ = lambda_\n",
    "        self.no_factors = no_factors\n",
    "        self.no_iterations = no_iterations\n",
    "        self.random_init = random_init\n",
    "        self.weights_mode = weights_mode\n",
    "        self.weight = weight\n",
    "        self.imputation_value = imputation_value\n",
    "        self.compute_ATOP = compute_ATOP\n",
    "\n",
    "\n",
    "        weighted_errors, ATOPs = self.optimize_rmse(beta)\n",
    "        self.ATOPv = np.max(ATOPs)\n",
    "        if(save_matrix):\n",
    "            self.save_matrices()\n",
    "\n",
    "#         self.plot_rmse(weighted_errors)\n",
    "        return np.max(ATOPs)\n",
    "    '''\n",
    "    PLOT AND EXPLORE\n",
    "    '''\n",
    "    def plot_rmse(self, weighted_errors):\n",
    "        plt.plot(np.log(weighted_errors), label=\"weighted error: \"+str(weighted_errors[-1]))\n",
    "        plt.ylabel(\"RMSE log scale\")\n",
    "        plt.xlabel(\"no iterations\")\n",
    "\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def explore(self):\n",
    "        print(\"Explore trainset\")\n",
    "        relevant = self.relevant\n",
    "        no_ratings = self.Trainset.shape[0]\n",
    "        no_missing = self.no_Users * self.no_Items - no_ratings\n",
    "        no_all = no_ratings + no_missing\n",
    "\n",
    "        sizes = [no_ratings, no_missing]\n",
    "        labels = [\"ratings\", \"missings\"]\n",
    "        colors = ['yellowgreen', 'lightskyblue']\n",
    "\n",
    "        #pozorovane vs. chybejici hodnoceni\n",
    "        plt.pie(sizes, labels=labels,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90, colors= colors)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        print(\"#{} rating, #{} missing\".format(no_ratings, no_missing))\n",
    "        print(\"Estimate offset w(m): \", no_ratings/no_missing)\n",
    "        ratings = self.Trainset[\"rating\"].values\n",
    "        print(\"Avg of ratings: \", ratings.mean())\n",
    "\n",
    "        #relevantni vs. irelevantni hodnoceni\n",
    "        no_relevant = np.sum(ratings>=relevant)\n",
    "        no_irelevant = np.sum(ratings<relevant)\n",
    "        labels = [\"relevant\", \"irrelevenat\"]\n",
    "\n",
    "        sizes = [no_relevant, no_irelevant]\n",
    "        colors = ['lightskyblue', 'lightcoral']\n",
    "        plt.pie(sizes, labels=labels,\n",
    "                autopct='%1.1f%%', shadow=True, startangle=90, colors= colors)\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"#{} relevant, #{} irelevant\".format(no_relevant, no_irelevant))\n",
    "\n",
    "        #rozlozeni popularity mezi items\n",
    "        items_popularity = np.sort(list(self.Item_pop.values()))[::-1]\n",
    "#         plt.bar(range(len(items_popularity)),items_popularity)\n",
    "        plt.xticks([])\n",
    "        plt.ylabel(\"# of item rating marked as relevant\")\n",
    "        plt.xlabel(\"item\")\n",
    "        plt.plot(items_popularity,'_')\n",
    "        plt.show()\n",
    "\n",
    "        #Histogram hodnoceni\n",
    "        plt.hist(self.Trainset['rating'].values,bins = len(set(self.Trainset['rating'])))\n",
    "        plt.xlabel(\"rating\")\n",
    "        plt.show()\n",
    "\n",
    "    def save_matrices(self):\n",
    "        d = now()\n",
    "        no_iterations, no_factors, lambda_ , weight, r_m = self.no_iterations, self.no_factors, self.lambda_, self.weight, self.imputation_value\n",
    "        ndataset, fold = self.ndataset, self.no_fold\n",
    "        if not os.path.exists(data_path+\"MATRICES/\"+ndataset):\n",
    "            os.makedirs(data_path+\"MATRICES/\"+ndataset)\n",
    "        if(self.compute_ATOP):\n",
    "            atop_suffix = \"_ATOP:\"+str(self.ATOPv)\n",
    "        else:\n",
    "            atop_suffix =\"\"\n",
    "\n",
    "        with open(data_path+\"MATRICES/\"+ndataset+\"/\"+ndataset+str(no_fold)+\"_model_f:\"+str(no_factors)+\"l:\"+str(lambda_)+\"w:\"+str(weight)+\"b:\"+str(self.beta)+\"r:\"+str(self.imputation_value)+atop_suffix+\".txt\", 'w') as f:\n",
    "            f.write(\"m \"+str(self.Users.shape[0])+\"\\n\")\n",
    "            f.write(\"n \"+str(self.Items.shape[0])+\"\\n\")\n",
    "            f.write(\"k \"+str(no_factors)+\"\\n\")\n",
    "\n",
    "            user_map = {v: k for k, v in self.user_map_dict.items()}\n",
    "            for idx, laten_vec in enumerate(self.Users):\n",
    "                string_idx = user_map[idx]\n",
    "                idf = self.users_str2int[string_idx]\n",
    "                f.write(\"p\"+str(idf)+\" \"+' '.join(list(laten_vec.astype('str')))+\"\\n\")\n",
    "\n",
    "            item_map = {v: k for k, v in self.item_map_dict.items()}\n",
    "            for idx, laten_vec in enumerate(self.Items):\n",
    "                string_idx = item_map[idx]\n",
    "                idf = self.items_str2int[string_idx]\n",
    "                f.write(\"q\"+str(idf)+\" \"+' '.join(list(laten_vec.astype('str')))+\"\\n\")\n",
    "\n",
    "        #print(\"************ UKLADANI MATICE \", now()-d)\n",
    "\n",
    "def NRANKs_u(args):\n",
    "    \"\"\"Vypocti normalizovany rank pro uzivatele.\n",
    "    args: user latentni vektor, user testovaci items, matice latentnich vektoru items\n",
    "    \"\"\"\n",
    "    user_vec, items, V = args\n",
    "\n",
    "    ratings_hat = np.dot(user_vec, V.T)\n",
    "    N = ratings_hat.shape[0]\n",
    "    ratings = ratings_hat[items]\n",
    "    nranks = np.array(list(map(lambda rating: np.sum(ratings_hat<rating), ratings)))/N\n",
    "\n",
    "    return nranks\n",
    "\n",
    "\n",
    "# # MAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     test_size, relevant = 0.0, 1\n",
    "#     SAVE_MATRIX = True\n",
    "#     ATOP = False\n",
    "#     #[0,1,2,3,4,5,6,7,8,9]\n",
    "#     for no_fold in [0,1,2,3,4,5,6,7,8,9]:  #iteruj pres testovaci slozky\n",
    "#         #odstran z datasetu testovaci users obsazene ve slozce\n",
    "#         ratings = dataset[~dataset.userId.isin(user_folds[no_fold])]\n",
    "#         trainset, testset = split_dataset(ratings.copy(),test_size = test_size, relevant = relevant)\n",
    "\n",
    "#         MFact = MatrixFactorization(trainset, testset, no_fold = no_fold , test_size = test_size, relevant = relevant, ndataset = ndataset,\n",
    "#                                     users_str2int= users_str2int, items_str2int = items_str2int)\n",
    "#         #[0, 0.0008, 0.001, 0.0015 ,0.002, 0.005, 0.01, 0.02]\n",
    "#         for lambda_ in [0, 0.001, 0.005, 0.008, 0.01]: #iteruj pres lambda\n",
    "#             #[1, 2, 5, 10, 30, 50, 100,200, 300],\n",
    "#             for no_factor in [30,100]: # iteruj pres delku latentnich vektoru\n",
    "#                 for no_iterations in [6]: #iteruj pres pocet iteraci alternating least square\n",
    "#                     #[-2,-1,-0.5,-0.2,0, 0.2, 0.5, 1, 2]\n",
    "#                     for beta in [0, 0.1, 0.2]:\n",
    "#                         for weight in [0.02, 0.05, 0.08, 0.1]:\n",
    "#                             for imputation_value in [0, 0.01]:\n",
    "#                                 for p in [5]:\n",
    "#                                     d = now()\n",
    "#                                     ATOP = MFact.optimaze(no_iterations = no_iterations,  lambda_ = lambda_, no_processes = p, no_factors = no_factor,\n",
    "#                                                           beta = beta,\n",
    "#                                                           weights_mode = \"AllRank-pop\", weight = weight, imputation_value = imputation_value,\n",
    "#                                                           random_init = True, mlprocessing = True, save_matrix = SAVE_MATRIX, compute_ATOP = ATOP)\n",
    "\n",
    "#                                     print(\"********** TIME \",p, now() - d)\n",
    "#                                     print(\"************************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AllRank-pop\n",
      "** Set weight:  0.08  to missing ratings , beta:  0  and avg weight  1.0 **\n",
      "** Surrogate missing rating values by imputation value:  -0.25  **\n",
      "*******************************\n",
      "Lambda:  0.01\n",
      "Impute value:  -0.25\n",
      "Weight:  0.08\n",
      "Beta:  0\n",
      "Factors:  30\n",
      "Number of processes:  6\n",
      "Number of iterations:  6\n",
      "*******************************\n",
      "UU  0:00:00.279898\n",
      "ITEM TIME  0:00:03.391908\n",
      "ITEM TIME  0:00:03.541103\n",
      "ITEM TIME  0:00:03.472428\n",
      "ITEM TIME  0:00:03.856966\n",
      "ITEM TIME  0:00:03.816240\n",
      "ITEM TIME  0:00:03.642107\n",
      "Item time 0:00:04.065407\n",
      "VV  0:00:00.008509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-10:\n",
      "Traceback (most recent call last):\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-11:\n",
      "Process Process-9:\n",
      "Process Process-12:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 285, in users_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 384, in solve\n",
      "    r = gufunc(a, b, signature=signature, extobj=extobj)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 287, in users_factor\n",
      "    U[u,:] = res.ravel()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 285, in users_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 285, in users_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 285, in users_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 384, in solve\n",
      "    r = gufunc(a, b, signature=signature, extobj=extobj)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 356, in solve\n",
      "    a, _ = _makearray(a)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 254, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-d9f52ca3abdd>\", line 285, in users_factor\n",
      "    res = np.linalg.solve(rM+reg,lM.T)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 358, in solve\n",
      "    _assertNdSquareness(a)\n",
      "  File \"/home/kuba/anaconda3/lib/python3.5/site-packages/numpy/linalg/linalg.py\", line 211, in _assertNdSquareness\n",
      "    if max(a.shape[-2:]) != min(a.shape[-2:]):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-73fd9fd1ac25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m                                                           \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                                                           \u001b[0mweights_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"AllRank-pop\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimputation_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputation_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                                                           random_init = True, mlprocessing = True, save_matrix = SAVE_MATRIX, compute_ATOP = ATOP)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-d9f52ca3abdd>\u001b[0m in \u001b[0;36moptimaze\u001b[1;34m(self, no_iterations, loss_function, lambda_, no_processes, no_factors, random_init, weights_mode, weight, imputation_value, beta, mlprocessing, save_matrix, compute_ATOP)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[0mweighted_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mATOPs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_rmse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mATOPv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mATOPs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d9f52ca3abdd>\u001b[0m in \u001b[0;36moptimize_rmse\u001b[1;34m(self, beta)\u001b[0m\n\u001b[0;32m    376\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"User time \"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d9f52ca3abdd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    376\u001b[0m                     \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"User time \"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a child process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'can only join a started process'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0m_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     49\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/kuba/anaconda3/lib/python3.5/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m                     \u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                     \u001b[1;31m# Child process not yet created. See #1731717\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_size, relevant = 0.0, 1\n",
    "SAVE_MATRIX = False\n",
    "ATOP = False\n",
    "no_fold = 0\n",
    "ratings = dataset[~dataset.userId.isin(user_folds[no_fold])]\n",
    "trainset, testset = split_dataset(ratings.copy(),test_size = test_size, relevant = relevant)\n",
    "\n",
    "MFact = MatrixFactorization(trainset, testset, no_fold = no_fold , test_size = test_size, relevant = relevant, ndataset = ndataset,\n",
    "                                    users_str2int= users_str2int, items_str2int = items_str2int)\n",
    "\n",
    "no_iterations = 6\n",
    "lambda_ = 0.01\n",
    "no_processes = 6\n",
    "no_factors = 30\n",
    "beta = 0\n",
    "weights_mode = \"AllRank-pop\"\n",
    "weight = 0.08\n",
    "imputation_value = -0.25\n",
    "random_init = True\n",
    "mlprocessing = True \n",
    "save_matrix = SAVE_MATRIX\n",
    "compute_ATOP = ATOP\n",
    "MFact.optimaze(no_iterations = no_iterations,  lambda_ = lambda_, no_processes = no_processes, no_factors = no_factors,\n",
    "                                                          beta = beta,\n",
    "                                                          weights_mode = \"AllRank-pop\", weight = weight, imputation_value = imputation_value,\n",
    "                                                          random_init = True, mlprocessing = True, save_matrix = SAVE_MATRIX, compute_ATOP = ATOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f MFact.users_factor MFact.users_factor(range(0,10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.solve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%lprun -f MFact.users_factor1 MFact.users_factor1(range(0,10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-74-0d429b504a33>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-74-0d429b504a33>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    9.4 4.8\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "0.306621 s 0.295404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = list(np.random.random(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.27 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 62.4 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.asmatrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 114 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.array(a).reshape(1,np.array(a).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 56.3 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.array([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 27.53 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 656 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.arange(100).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 26.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 3: 670 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit len(np.arange(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 25.53 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 70.4 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit MFact.User_Items[200]['weights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-47ab7821b2de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "a.append(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a  = np.random.rand(100*1000).reshape(100,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(a*a == np.multiply(a,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 49.7 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 48.9 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit np.multiply(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
